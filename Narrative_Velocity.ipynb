{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPSYz6TI45IIYndabs2cuA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhussn/Bias-Velocity-Research-Project/blob/main/Narrative_Velocity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch numpy"
      ],
      "metadata": {
        "id": "jq6cyKnAsZBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Web Scrape Articles\n",
        "import feedparser\n",
        "from newspaper import Article\n",
        "import csv\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import requests\n",
        "import re\n",
        "\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (compatible; NewsScraper/1.0; +http://yourdomain.com)'\n",
        "}\n",
        "\n",
        "topics = {\n",
        "    \"immigration\": {\n",
        "        \"conservative\": {\n",
        "            \"Fox News Politics\": \"http://feeds.foxnews.com/foxnews/politics\",\n",
        "            \"Fox News US Immigration\": \"https://www.foxnews.com/category/us/immigration/feed\",\n",
        "            \"The Daily Caller\": \"https://dailycaller.com/feed/\",\n",
        "            \"The Blaze\": \"https://www.theblaze.com/rss\",\n",
        "            \"Breitbart\": \"http://feeds.feedburner.com/breitbart\",\n",
        "            \"Breitbart Politics\": \"https://www.breitbart.com/politics/feed/\",\n",
        "            \"National Review\": \"https://www.nationalreview.com/feed/\",\n",
        "            \"The Washington Times\": \"https://www.washingtontimes.com/rss/feed/\",\n",
        "            \"The Epoch Times\": \"https://www.theepochtimes.com/feed.xml\",\n",
        "            \"Newsmax\": \"https://www.newsmax.com/rss/\",\n",
        "            \"Townhall\": \"https://townhall.com/rss/rss.xml\",\n",
        "            \"The Federalist\": \"https://thefederalist.com/feed/\",\n",
        "            \"Daily Wire\": \"https://www.dailywire.com/rss\",\n",
        "            \"One America News\": \"https://www.oann.com/feed/\",\n",
        "            \"Washington Examiner\": \"https://www.washingtonexaminer.com/feed/rss\",\n",
        "            \"American Thinker\": \"https://www.americanthinker.com/rss.xml\",\n",
        "            \"The American Conservative\": \"https://www.theamericanconservative.com/feed/\",\n",
        "            \"The Daily Signal\": \"https://www.dailysignal.com/feed/\"\n",
        "        },\n",
        "        \"moderate\": {\n",
        "            \"Reuters\": \"https://www.reutersagency.com/feed/?best-topics=politics\",\n",
        "            \"Associated Press Top News\": \"https://apnews.com/apf-topnews?format=rss\",\n",
        "            \"Associated Press Politics\": \"https://apnews.com/apf-topnews?format=rss\",\n",
        "            \"NPR General\": \"https://www.npr.org/rss/rss.php?id=1001\",\n",
        "            \"NPR Politics\": \"https://www.npr.org/rss/rss.php?id=1014\",\n",
        "            \"USA Today Nation\": \"https://rssfeeds.usatoday.com/UsatodaycomNation-TopStories\",\n",
        "            \"USA Today Politics\": \"https://rssfeeds.usatoday.com/UsatodaycomPolitics-TopStories\",\n",
        "            \"PBS NewsHour\": \"https://www.pbs.org/newshour/feed/\",\n",
        "            \"PBS Newshour Politics\": \"https://www.pbs.org/newshour/politics/feed/\",\n",
        "            \"Bloomberg Politics\": \"https://www.bloomberg.com/feed/podcast/politics.xml\",\n",
        "            \"Politico\": \"https://www.politico.com/rss/politics08.xml\",\n",
        "            \"The Hill\": \"https://thehill.com/rss/syndicator/19109\",\n",
        "            \"CBS News Politics\": \"https://www.cbsnews.com/latest/rss/politics\",\n",
        "            \"ABC News Politics\": \"https://abcnews.go.com/abcnews/politicsheadlines\",\n",
        "            \"The Wall Street Journal General\": \"https://www.wsj.com/xml/rss/3_7014.xml\",\n",
        "            \"The Wall Street Journal Politics\": \"https://www.wsj.com/xml/rss/3_7014.xml\",\n",
        "            \"Financial Times\": \"https://www.ft.com/?format=rss\",\n",
        "            \"The Christian Science Monitor\": \"https://www.csmonitor.com/feeds/rss\",\n",
        "            \"Axios Politics\": \"https://www.axios.com/feed.xml\",\n",
        "            \"BBC News US & Canada\": \"http://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml\",\n",
        "            \"Al Jazeera English\": \"https://www.aljazeera.com/xml/rss/all.xml\"\n",
        "        },\n",
        "        \"liberal\": {\n",
        "            \"CNN Politics\": \"http://rss.cnn.com/rss/edition_politics.rss\",\n",
        "            \"CNN Immigration\": \"http://rss.cnn.com/rss/edition_us_immigration.rss\",\n",
        "            \"The Guardian Immigration\": \"https://www.theguardian.com/us-news/immigration/rss\",\n",
        "            \"Mother Jones\": \"https://www.motherjones.com/feed/\",\n",
        "            \"MSNBC Latest\": \"http://www.msnbc.com/feeds/latest\",\n",
        "            \"HuffPost Politics\": \"https://www.huffpost.com/section/politics/feed\",\n",
        "            \"Vox\": \"https://www.vox.com/rss/index.xml\",\n",
        "            \"Daily Kos\": \"https://www.dailykos.com/rss/main\",\n",
        "            \"Salon\": \"https://www.salon.com/feed/\",\n",
        "            \"The New Republic\": \"https://newrepublic.com/rss.xml\",\n",
        "            \"The Atlantic\": \"https://www.theatlantic.com/feed/all/\",\n",
        "            \"Slate\": \"https://slate.com/feed\",\n",
        "            \"ThinkProgress (Archive)\": \"https://archive.thinkprogress.org/feed/\",\n",
        "            \"The Nation\": \"https://www.thenation.com/feed/\",\n",
        "            \"Common Dreams\": \"https://www.commondreams.org/feed/rss.xml\",\n",
        "            \"Raw Story\": \"https://www.rawstory.com/rss/\",\n",
        "            \"Truthout\": \"https://truthout.org/feed/\",\n",
        "            \"Democracy Now\": \"https://www.democracynow.org/democracynow.rss\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "MAX_ARTICLES_PER_IDEOLOGY = 3\n",
        "MAX_ARTICLES_PER_OUTLET = 4\n",
        "\n",
        "KEYWORDS = [\n",
        "    \"ice\",\n",
        "    \"immigration and customs enforcement\",\n",
        "    \"immigration enforcement\",\n",
        "    \"deportation\",\n",
        "    \"border patrol\",\n",
        "    \"customs and border protection\",\n",
        "    \"cbp\",\n",
        "    \"detention center\",\n",
        "    \"immigrant detention\",\n",
        "    \"immigration raids\",\n",
        "    \"immigration crackdown\",\n",
        "    \"immigration policy\",\n",
        "    \"immigration reform\",\n",
        "    \"immigration laws\",\n",
        "    \"immigration agents\",\n",
        "    \"immigration officials\",\n",
        "    \"border security\",\n",
        "    \"migrant detention\",\n",
        "    \"immigration detention facility\",\n",
        "    \"immigration court\",\n",
        "    \"immigrant rights\",\n",
        "    \"family separation\",\n",
        "    \"sanctuary cities\",\n",
        "    \"deportee\",\n",
        "    \"ice agents\",\n",
        "    \"undocumented immigrants\",\n",
        "    \"migrant caravan\",\n",
        "    \"asylum seekers\",\n",
        "    \"border crossing\",\n",
        "    \"illegal immigration\",\n",
        "    \"immigration ban\",\n",
        "    \"visa policy\",\n",
        "    \"naturalization\",\n",
        "    \"immigration detention center\",\n",
        "    \"immigration raid\",\n",
        "    \"deportee\",\n",
        "    \"migration policy\",\n",
        "    \"refugee status\",\n",
        "    \"immigration\",\n",
        "    \"border\",\n",
        "    \"border security\",\n",
        "    \"asylum\",\n",
        "    \"visa\",\n",
        "    \"green card\",\n",
        "    \"immigration reform\",\n",
        "    \"deportation\",\n",
        "    \"ICE\",\n",
        "    \"CBP\",\n",
        "    \"citizenship\",\n",
        "    \"migrant policy\",\n",
        "    \"migrant caravan\",\n",
        "    \"border wall\",\n",
        "    \"refugee\",\n",
        "    \"work permit\",\n",
        "    \"naturalization\",\n",
        "    \"detention center\",\n",
        "    \"family separation\",\n",
        "    \"Title 42\",\n",
        "    \"parole program\",\n",
        "    \"illegal immigration\",\n",
        "    \"mass migration\",\n",
        "    \"undocumented immigrants\",\n",
        "    \"sanctuary city\",\n",
        "    \"sanctuary cities\",\n",
        "    \"amnesty\",\n",
        "    \"open borders\",\n",
        "    \"migrant surge\",\n",
        "    \"immigration\",\n",
        "    \"border\",\n",
        "    \"border security\",\n",
        "    \"asylum\",\n",
        "    \"visa\",\n",
        "    \"green card\",\n",
        "    \"immigration reform\",\n",
        "    \"deportation\",\n",
        "    \"ICE\",\n",
        "    \"CBP\",\n",
        "    \"citizenship\",\n",
        "    \"migrant policy\",\n",
        "    \"migrant caravan\",\n",
        "    \"border wall\",\n",
        "    \"refugee\",\n",
        "    \"work permit\",\n",
        "    \"naturalization\",\n",
        "    \"detention center\",\n",
        "    \"family separation\",\n",
        "    \"Title 42\",\n",
        "    \"parole program\",\n",
        "    \"illegal immigration\",\n",
        "    \"mass migration\",\n",
        "    \"undocumented immigrants\",\n",
        "    \"sanctuary city\",\n",
        "    \"sanctuary cities\",\n",
        "    \"amnesty\",\n",
        "    \"open borders\",\n",
        "    \"migrant surge\",\n",
        "    \"border patrol\",\n",
        "    \"immigration raid\",\n",
        "    \"ICE raid\",\n",
        "    \"removal proceedings\",\n",
        "    \"immigration detention\",\n",
        "    \"immigration enforcement\",\n",
        "    \"immigration crackdown\",\n",
        "    \"expedited removal\",\n",
        "    \"temporary protected status\",\n",
        "    \"TPS\",\n",
        "    \"DACA\",\n",
        "    \"Dreamers\",\n",
        "    \"E-Verify\",\n",
        "    \"immigration court\",\n",
        "    \"customs enforcement\",\n",
        "    \"ICE facility\",\n",
        "    \"immigration prison\",\n",
        "    \"deferred action\",\n",
        "    \"catch and release\",\n",
        "    \"migrant processing\",\n",
        "    \"detention facility\",\n",
        "    \"ICE detention\",\n",
        "\n",
        "]\n",
        "\n",
        "KEYWORDS = [k.lower() for k in KEYWORDS]\n",
        "\n",
        "def url_exists(url):\n",
        "    try:\n",
        "        response = requests.head(url, allow_redirects=True, timeout=5, headers=HEADERS)\n",
        "        if response.status_code == 200:\n",
        "            return True\n",
        "        response = requests.get(url, stream=True, timeout=5, headers=HEADERS)\n",
        "        return response.status_code == 200\n",
        "    except requests.RequestException:\n",
        "        return False\n",
        "\n",
        "def to_naive(dt):\n",
        "    if dt is None:\n",
        "        return None\n",
        "    if dt.tzinfo is not None:\n",
        "        return dt.astimezone(timezone.utc).replace(tzinfo=None)\n",
        "    return dt\n",
        "\n",
        "def is_recent(publish_date, days=30):\n",
        "    publish_date_naive = to_naive(publish_date)\n",
        "    now_naive = datetime.utcnow()\n",
        "    if not publish_date_naive:\n",
        "        return False\n",
        "    return publish_date_naive >= now_naive - timedelta(days=days)\n",
        "\n",
        "def contains_keyword_in_title(title, keywords):\n",
        "    title = title.lower()\n",
        "    for kw in keywords:\n",
        "        pattern = r'\\b' + re.escape(kw) + r'\\b'\n",
        "        if re.search(pattern, title):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "output_rows = []\n",
        "topic = \"immigration\"\n",
        "ideologies = topics[topic]\n",
        "\n",
        "counts = {ideo: 0 for ideo in ideologies}\n",
        "outlet_counts = {ideo: {outlet: 0 for outlet in outlets} for ideo, outlets in ideologies.items()}\n",
        "\n",
        "print(f\"Starting scraping articles on '{topic}' topic...\")\n",
        "\n",
        "# Loop until each ideology reaches max article count\n",
        "while any(counts[ideo] < MAX_ARTICLES_PER_IDEOLOGY for ideo in counts):\n",
        "    for ideology, outlets in ideologies.items():\n",
        "        if counts[ideology] >= MAX_ARTICLES_PER_IDEOLOGY:\n",
        "            continue\n",
        "\n",
        "        for outlet, feed_url in outlets.items():\n",
        "            if outlet_counts[ideology][outlet] >= MAX_ARTICLES_PER_OUTLET:\n",
        "                continue\n",
        "\n",
        "            print(f\"Fetching feed: {outlet} ({ideology})\")\n",
        "            feed = feedparser.parse(feed_url)\n",
        "            for entry in feed.entries:\n",
        "                if counts[ideology] >= MAX_ARTICLES_PER_IDEOLOGY or outlet_counts[ideology][outlet] >= MAX_ARTICLES_PER_OUTLET:\n",
        "                    break\n",
        "\n",
        "                url = entry.link\n",
        "                if not url_exists(url):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    article = Article(url)\n",
        "                    article.download()\n",
        "                    article.parse()\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "                # Get publish date\n",
        "                publish_date = None\n",
        "                if hasattr(article, 'publish_date') and article.publish_date:\n",
        "                    publish_date = article.publish_date\n",
        "                elif hasattr(entry, 'published_parsed'):\n",
        "                    publish_date = datetime(*entry.published_parsed[:6])\n",
        "\n",
        "                if not is_recent(publish_date):\n",
        "                    continue\n",
        "\n",
        "                title = article.title if article and article.title else (entry.title if hasattr(entry, 'title') else \"\")\n",
        "                sample_text = article.text[:10000].strip() if article and article.text else \"\"\n",
        "\n",
        "                if not sample_text:\n",
        "                    continue\n",
        "\n",
        "                # STRICT keyword check only in title (whole word matching)\n",
        "                if not contains_keyword_in_title(title, KEYWORDS):\n",
        "                    continue\n",
        "\n",
        "                # Skip duplicates\n",
        "                if any(row[\"url\"] == url for row in output_rows):\n",
        "                    continue\n",
        "\n",
        "                datetime_str = publish_date.strftime(\"%Y-%m-%d %H:%M\") if publish_date else \"\"\n",
        "\n",
        "                output_rows.append({\n",
        "                    \"topic\": topic,\n",
        "                    \"outlet\": outlet,\n",
        "                    \"datetime\": datetime_str,\n",
        "                    \"title\": title,\n",
        "                    \"url\": url,\n",
        "                    \"sample_text\": sample_text,\n",
        "                    \"ideological_stance\": ideology,\n",
        "                    \"factual_grounding\": \"\",\n",
        "                    \"framing_choices\": \"\",\n",
        "                    \"emotional_tone\": \"\",\n",
        "                    \"source_transparency\": \"\"\n",
        "                })\n",
        "\n",
        "                counts[ideology] += 1\n",
        "                outlet_counts[ideology][outlet] += 1\n",
        "\n",
        "                print(f\"Added article ({counts[ideology]}/{MAX_ARTICLES_PER_IDEOLOGY}) from {outlet} ({ideology})\")\n",
        "\n",
        "            if counts[ideology] >= MAX_ARTICLES_PER_IDEOLOGY:\n",
        "                print(f\"Reached max articles for {ideology}\")\n",
        "\n",
        "print(\"Scraping done. Saving to CSV...\")\n",
        "\n",
        "csv_columns = [\n",
        "    \"topic\", \"outlet\", \"datetime\", \"title\", \"url\", \"sample_text\",\n",
        "    \"ideological_stance\", \"factual_grounding\", \"framing_choices\", \"emotional_tone\", \"source_transparency\"\n",
        "]\n",
        "\n",
        "with open(\"news_bias_articles.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(output_rows)\n",
        "\n",
        "print(\"Done! Articles saved to news_bias_articles.csv\")"
      ],
      "metadata": {
        "id": "fgqyjnxKNIEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Bias Scoring\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "OUTLET_TO_IDEOLOGY = {\n",
        "    # Conservative outlets (matching your new keys exactly)\n",
        "    \"Fox News Politics\": \"conservative\",\n",
        "    \"Fox News US Immigration\": \"conservative\",\n",
        "    \"The Daily Caller\": \"conservative\",\n",
        "    \"The Blaze\": \"conservative\",\n",
        "    \"Breitbart\": \"conservative\",\n",
        "    \"Breitbart Politics\": \"conservative\",\n",
        "    \"National Review\": \"conservative\",\n",
        "    \"The Washington Times\": \"conservative\",\n",
        "    \"The Epoch Times\": \"conservative\",\n",
        "    \"Newsmax\": \"conservative\",\n",
        "    \"Townhall\": \"conservative\",\n",
        "    \"The Federalist\": \"conservative\",\n",
        "    \"Daily Wire\": \"conservative\",\n",
        "    \"One America News\": \"conservative\",\n",
        "    \"Washington Examiner\": \"conservative\",\n",
        "    \"American Thinker\": \"conservative\",\n",
        "    \"The American Conservative\": \"conservative\",\n",
        "    \"The Daily Signal\": \"conservative\",\n",
        "\n",
        "    # Moderate outlets\n",
        "    \"Reuters\": \"moderate\",\n",
        "    \"Associated Press Top News\": \"moderate\",\n",
        "    \"Associated Press Politics\": \"moderate\",\n",
        "    \"NPR General\": \"moderate\",\n",
        "    \"NPR Politics\": \"moderate\",\n",
        "    \"USA Today Nation\": \"moderate\",\n",
        "    \"USA Today Politics\": \"moderate\",\n",
        "    \"PBS NewsHour\": \"moderate\",\n",
        "    \"PBS Newshour Politics\": \"moderate\",\n",
        "    \"Bloomberg Politics\": \"moderate\",\n",
        "    \"Politico\": \"moderate\",\n",
        "    \"The Hill\": \"moderate\",\n",
        "    \"CBS News Politics\": \"moderate\",\n",
        "    \"ABC News Politics\": \"moderate\",\n",
        "    \"The Wall Street Journal General\": \"moderate\",\n",
        "    \"The Wall Street Journal Politics\": \"moderate\",\n",
        "    \"Financial Times\": \"moderate\",\n",
        "    \"The Christian Science Monitor\": \"moderate\",\n",
        "    \"Axios Politics\": \"moderate\",\n",
        "    \"BBC News US & Canada\": \"moderate\",\n",
        "    \"Al Jazeera English\": \"moderate\",\n",
        "\n",
        "    # Liberal outlets\n",
        "    \"CNN Politics\": \"liberal\",\n",
        "    \"CNN Immigration\": \"liberal\",\n",
        "    \"The Guardian Immigration\": \"liberal\",\n",
        "    \"Mother Jones\": \"liberal\",\n",
        "    \"MSNBC Latest\": \"liberal\",\n",
        "    \"HuffPost Politics\": \"liberal\",\n",
        "    \"Vox\": \"liberal\",\n",
        "    \"Daily Kos\": \"liberal\",\n",
        "    \"Salon\": \"liberal\",\n",
        "    \"The New Republic\": \"liberal\",\n",
        "    \"The Atlantic\": \"liberal\",\n",
        "    \"Slate\": \"liberal\",\n",
        "    \"ThinkProgress (Archive)\": \"liberal\",\n",
        "    \"The Nation\": \"liberal\",\n",
        "    \"Common Dreams\": \"liberal\",\n",
        "    \"Raw Story\": \"liberal\",\n",
        "    \"Truthout\": \"liberal\",\n",
        "    \"Democracy Now\": \"liberal\",\n",
        "}\n",
        "\n",
        "\n",
        "# Bias dimensions, model returns lowercase labels, so lowercase here\n",
        "BIAS_DIMENSIONS = {\n",
        "    \"ideological_stance\": [\"left\", \"center\", \"right\"],\n",
        "    \"factual_grounding\": [\"low\", \"medium\", \"high\"],\n",
        "    \"framing_choices\": [\"biased\", \"balanced\", \"unbiased\"],\n",
        "    \"emotional_tone\": [\"neutral\", \"mild\", \"inflammatory\"],\n",
        "    \"source_transparency\": [\"opaque\", \"moderate\", \"transparent\"]\n",
        "}\n",
        "\n",
        "# Load zero-shot classifier pipeline once\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Map model labels to numeric ideology scores\n",
        "model_label_to_score = {\"left\": 0, \"center\": 50, \"right\": 100}\n",
        "# Map outlet labels (lowercase) to numeric ideology scores to match your ideology labels in data\n",
        "outlet_label_to_score = {\"liberal\": 0, \"moderate\": 50, \"conservative\": 100}\n",
        "\n",
        "def score_text(text, max_retries=3):\n",
        "    results = {}\n",
        "    for dim, labels in BIAS_DIMENSIONS.items():\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                res = classifier(text, labels, multi_label=False)\n",
        "                returned_labels = [label.lower() for label in res['labels']]\n",
        "                probs = np.array(res['scores'])\n",
        "                probs = probs / probs.sum()  # normalize\n",
        "\n",
        "                label_to_score = {label: idx * 50 for idx, label in enumerate(labels)}\n",
        "                weighted_score = sum(probs[i] * label_to_score[returned_labels[i]] for i in range(len(labels)))\n",
        "\n",
        "                results[dim] = round(weighted_score, 2)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Error scoring '{dim}', attempt {attempt + 1}/{max_retries}: {e}\")\n",
        "                time.sleep(1)\n",
        "                if attempt == max_retries - 1:\n",
        "                    results[dim] = None\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    df = pd.read_csv(\"news_bias_articles.csv\")\n",
        "    total_rows = len(df)\n",
        "    print(f\"Total articles to process: {total_rows}\")\n",
        "\n",
        "    # Do NOT lowercase outlet names â€” keep as is for matching\n",
        "    df['outlet'] = df['outlet'].str.strip()  # Just strip spaces, no lowercase\n",
        "\n",
        "    # Add columns if missing\n",
        "    for col in list(BIAS_DIMENSIONS.keys()) + [\"ideology_label\", \"combined_ideological_stance\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "\n",
        "    weight_outlet = 0.7  # weight of outlet ideology in final score\n",
        "    weight_model = 0.3   # weight of model predicted ideology in final score\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        text = row.get('sample_text', \"\")\n",
        "        if pd.isna(text) or not text.strip():\n",
        "            print(f\"Skipping empty text at index {idx}\")\n",
        "            continue\n",
        "\n",
        "        outlet = row.get('outlet', \"\")\n",
        "        outlet_label = OUTLET_TO_IDEOLOGY.get(outlet)\n",
        "        if outlet_label is None:\n",
        "            print(f\"Unknown outlet ideology for '{outlet}' at index {idx}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Store outlet ideology label (lowercase)\n",
        "        df.at[idx, \"ideology_label\"] = outlet_label\n",
        "\n",
        "        # Get outlet ideology numeric score\n",
        "        outlet_score = outlet_label_to_score[outlet_label]\n",
        "\n",
        "        # Get model scores for all dimensions, including ideological_stance\n",
        "        scores = score_text(text.strip())\n",
        "        for dim in BIAS_DIMENSIONS.keys():\n",
        "            df.at[idx, dim] = scores.get(dim)\n",
        "\n",
        "        # Combine outlet and model ideological stance scores\n",
        "        model_score = scores.get(\"ideological_stance\")\n",
        "        if model_score is None:\n",
        "            combined_score = outlet_score  # fallback if model failed\n",
        "        else:\n",
        "            combined_score = round(weight_outlet * outlet_score + weight_model * model_score, 2)\n",
        "\n",
        "        df.at[idx, \"combined_ideological_stance\"] = combined_score\n",
        "\n",
        "        if (idx + 1) % 10 == 0 or (idx + 1) == total_rows:\n",
        "            print(f\"Processed {idx + 1}/{total_rows} ({(idx + 1) / total_rows * 100:.1f}%)\")\n",
        "\n",
        "    df.to_csv(\"news_bias_articles_scored.csv\", index=False)\n",
        "    print(\"Saved scored CSV as news_bias_articles_scored.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "OcRt3ILzBNVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Set Narrative Clusters\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def narrative_clustering_and_labeling(\n",
        "    input_csv=\"news_bias_articles_scored.csv\",\n",
        "    output_csv=\"news_bias_articles_clustered_labeled.csv\",\n",
        "    n_clusters=3\n",
        "):\n",
        "    # Load scored CSV with ideological_stance scores\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Load Sentence-BERT model for embeddings\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Initialize cluster assignment lists\n",
        "    cluster_ids = [-1] * len(df)\n",
        "    cluster_labels = [None] * len(df)\n",
        "\n",
        "    print(f\"Clustering articles within each topic into {n_clusters} clusters each...\")\n",
        "\n",
        "    # Process each topic separately\n",
        "    for topic in df['topic'].unique():\n",
        "        subset_idx = df[df['topic'] == topic].index.tolist()\n",
        "        texts = df.loc[subset_idx, 'sample_text'].fillna(\"\").tolist()\n",
        "        ideological_scores = df.loc[subset_idx, 'ideological_stance'].fillna(50).tolist()  # Default center if missing\n",
        "\n",
        "        # Filter out empty texts (no embeddings for empty)\n",
        "        valid_indices = [i for i, t in enumerate(texts) if t.strip() != \"\"]\n",
        "        if len(valid_indices) == 0:\n",
        "            print(f\"No valid texts for topic '{topic}', skipping.\")\n",
        "            continue\n",
        "\n",
        "        texts_nonempty = [texts[i] for i in valid_indices]\n",
        "        ideology_nonempty = [ideological_scores[i] for i in valid_indices]\n",
        "\n",
        "        print(f\"Computing embeddings for topic '{topic}' with {len(texts_nonempty)} articles...\")\n",
        "        embeddings = model.encode(texts_nonempty, show_progress_bar=True)\n",
        "\n",
        "        # Combine embeddings with ideological stance as additional feature\n",
        "        ideology_array = np.array(ideology_nonempty).reshape(-1, 1)\n",
        "        combined_features = np.hstack([embeddings, ideology_array])\n",
        "\n",
        "        n_clust = min(n_clusters, len(texts_nonempty))\n",
        "        print(f\"Clustering topic '{topic}' into {n_clust} clusters using embeddings + ideology...\")\n",
        "        kmeans = KMeans(n_clusters=n_clust, random_state=42)\n",
        "        labels = kmeans.fit_predict(combined_features)\n",
        "\n",
        "        # Assign cluster IDs back to main dataframe indices for valid texts\n",
        "        for i, label in zip(valid_indices, labels):\n",
        "            cluster_ids[subset_idx[i]] = label\n",
        "\n",
        "        # Create temporary df slice with cluster info to calculate cluster means\n",
        "        sub_df = df.loc[subset_idx].copy()\n",
        "        sub_df['cluster_id'] = -1\n",
        "        for i, label in zip(valid_indices, labels):\n",
        "            sub_df.at[subset_idx[i], 'cluster_id'] = label\n",
        "\n",
        "        # Calculate mean ideological_stance per cluster for label mapping\n",
        "        cluster_means = sub_df.groupby('cluster_id')['ideological_stance'].mean().dropna()\n",
        "        sorted_clusters = cluster_means.sort_values().index.tolist()\n",
        "\n",
        "        # Assign human-readable bias labels depending on cluster count\n",
        "        bias_labels_map = {\n",
        "            3: [\"Liberal\", \"Unbiased\", \"Conservative\"],\n",
        "            2: [\"Liberal\", \"Conservative\"],\n",
        "            1: [\"Unbiased\"]\n",
        "        }\n",
        "        bias_labels = bias_labels_map.get(len(sorted_clusters), [f\"Cluster_{i}\" for i in range(len(sorted_clusters))])\n",
        "        cluster_label_map = {cid: bias_labels[i] for i, cid in enumerate(sorted_clusters)}\n",
        "\n",
        "        print(f\"Topic '{topic}' cluster label mapping: {cluster_label_map}\")\n",
        "\n",
        "        # Assign cluster labels to all articles in topic\n",
        "        for idx in subset_idx:\n",
        "            c_id = cluster_ids[idx]\n",
        "            cluster_labels[idx] = cluster_label_map.get(c_id, None)\n",
        "\n",
        "    # Add cluster info columns to DataFrame\n",
        "    df['cluster_id'] = cluster_ids\n",
        "    df['cluster_label'] = cluster_labels\n",
        "\n",
        "    # Save output CSV in current working directory\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Saved clustered and labeled articles to {output_csv}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    narrative_clustering_and_labeling()\n"
      ],
      "metadata": {
        "id": "ucqElkwRBR98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def score_to_label(score):\n",
        "    if pd.isna(score):\n",
        "        return \"Unknown\"\n",
        "    elif score <= 33:\n",
        "        return \"Liberal\"\n",
        "    elif score <= 66:\n",
        "        return \"Moderate\"\n",
        "    else:\n",
        "        return \"Conservative\"\n",
        "\n",
        "def analyze_velocity(input_csv=\"news_bias_articles_scored.csv\"):\n",
        "    # Load data with datetime parsing\n",
        "    df = pd.read_csv(input_csv, parse_dates=['datetime'])\n",
        "\n",
        "    # Map numeric ideological stance to label\n",
        "    df['combined_ideology_label'] = df['ideological_stance'].apply(score_to_label)\n",
        "\n",
        "    # Clean dataset\n",
        "    df = df.dropna(subset=['datetime', 'combined_ideology_label', 'outlet'])\n",
        "\n",
        "    # Sort by datetime ascending\n",
        "    df = df.sort_values('datetime')\n",
        "\n",
        "    print(f\"Total articles analyzed: {len(df)}\\n\")\n",
        "\n",
        "    # === Per ideology summary ===\n",
        "    first_article_times = {}\n",
        "    for ideology in ['Liberal', 'Moderate', 'Conservative']:\n",
        "        sub = df[df['combined_ideology_label'] == ideology]\n",
        "        if len(sub) == 0:\n",
        "            print(f\"No articles found for ideology: {ideology}\\n\")\n",
        "            continue\n",
        "\n",
        "        start_time = sub['datetime'].min()\n",
        "        end_time = sub['datetime'].max()\n",
        "        count = len(sub)\n",
        "        first_article_times[ideology] = start_time\n",
        "\n",
        "        print(f\"Ideology: {ideology}\")\n",
        "        print(f\"  Articles: {count}\")\n",
        "        print(f\"  Time range: {start_time} to {end_time}\")\n",
        "        print(f\"  First 3 articles:\")\n",
        "        print(sub[['datetime', 'outlet', 'title']].head(3).to_string(index=False))\n",
        "        print(f\"  Last 3 articles:\")\n",
        "        print(sub[['datetime', 'outlet', 'title']].tail(3).to_string(index=False))\n",
        "        print()\n",
        "\n",
        "    # === Publication counts per day per ideology ===\n",
        "    df['date'] = df['datetime'].dt.date\n",
        "    counts = df.groupby(['date', 'combined_ideology_label']).size().unstack(fill_value=0)\n",
        "    print(\"Publication counts per day per ideology:\")\n",
        "    print(counts)\n",
        "    print()\n",
        "\n",
        "    # === Calculate lag times between first article publications (in hours) per ideology ===\n",
        "    print(\"Lag times between first article publications (hours):\")\n",
        "    ideologies = ['Liberal', 'Moderate', 'Conservative']\n",
        "    for i in range(len(ideologies)):\n",
        "        for j in range(i+1, len(ideologies)):\n",
        "            a, b = ideologies[i], ideologies[j]\n",
        "            if a in first_article_times and b in first_article_times:\n",
        "                lag = (first_article_times[b] - first_article_times[a]).total_seconds() / 3600\n",
        "                print(f\"  {a} -> {b}: {lag:.2f} hours\")\n",
        "    print()\n",
        "\n",
        "    # === Per outlet first article times (to find initiators) ===\n",
        "    print(\"First article publication times per outlet:\")\n",
        "    outlets = df['outlet'].unique()\n",
        "    outlet_first_times = {}\n",
        "    for outlet in outlets:\n",
        "        out_sub = df[df['outlet'] == outlet]\n",
        "        first_time = out_sub['datetime'].min()\n",
        "        outlet_first_times[outlet] = first_time\n",
        "        print(f\"  {outlet}: {first_time}\")\n",
        "    print()\n",
        "\n",
        "    # === Visualization: Timeline scatter plot with ideological stance ===\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    sns.scatterplot(\n",
        "        data=df,\n",
        "        x='datetime',\n",
        "        y='ideological_stance',\n",
        "        hue='combined_ideology_label',\n",
        "        style='combined_ideology_label',\n",
        "        palette={'Liberal':'blue', 'Moderate':'green', 'Conservative':'red'},\n",
        "        s=100,\n",
        "        alpha=0.7\n",
        "    )\n",
        "    plt.title(\"Publication Timeline: Ideological Stance Over Time\")\n",
        "    plt.xlabel(\"Publication DateTime\")\n",
        "    plt.ylabel(\"Ideological Stance Score (0=Liberal, 100=Conservative)\")\n",
        "    plt.legend(title=\"Ideology\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # === Additional Visualization: Article count over time by ideology and outlet ===\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    sns.lineplot(\n",
        "        data=df,\n",
        "        x='date',\n",
        "        y=df.groupby(['date', 'combined_ideology_label']).size().reindex(df['date']).values,\n",
        "        hue='combined_ideology_label',\n",
        "        palette={'Liberal':'blue', 'Moderate':'green', 'Conservative':'red'}\n",
        "    )\n",
        "    plt.title(\"Daily Publication Counts by Ideology\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Number of Articles\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyze_velocity()"
      ],
      "metadata": {
        "id": "B3MRBeb-5Z7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}